DATA:
  dataset: joint_data
  data_root: /mnt/Datasets/joint_data
  wav_path: wav
  vertices_path: npz
  template_file: flame_model/assets/FLAME_with_eye.pt
  train_subjects: English 

NETWORK:
  arch: stage2
  in_dim: 58
  hidden_size: 512
  num_hidden_layers: 6
  num_attention_heads: 8
  intermediate_size: 1536
  window_size: 1
  quant_factor: 0
  face_quan_num: 16
  neg: 0.2
  autoencoder: stage1
  INaffine: False
  style_emb_method: nnemb # onehot or nnemb
  interactive_window: 50 # Number of frames to be used for interactive window
  hop_length: 640 # Number of audio samples per frame in your data
  
VQuantizer:
  n_embed: 256
  zquant_dim: 64

PREDICTOR:
  feature_dim: 1024 
  blendshapes_dim: 58
  device: cuda
  period: 25
  vqvae_pretrained_path: ./checkpoints/model_s1.pth.tar
  wav2vec2model_path: utter-project/mHuBERT-147 
  teacher_forcing: True
  num_layers: 4 
  n_head: 4 
  batch_size: 1

DEMO:
  model_path: ./checkpoints/model_s2.pth.tar
  condition: English
  subject: id
  demo_wav_dir_path: demo/input/
  demo_output_path: demo/output/
  fps: 25
  background_black: True 