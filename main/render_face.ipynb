{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/fasttalk/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# change base folder\n",
    "os.chdir('../')\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from flame_model.FLAME import FLAMEModel\n",
    "from renderer.renderer import Renderer\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "from pytorch3d.transforms import matrix_to_euler_angles\n",
    "import subprocess\n",
    "import tempfile \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import wandb\n",
    "import glob\n",
    "from models.stage2 import CodeTalker\n",
    "import yaml\n",
    "from models import get_model\n",
    "from base.baseTrainer import load_state_dict\n",
    "from types import SimpleNamespace\n",
    "from transformers import AutoProcessor, Wav2Vec2Processor, Wav2Vec2FeatureExtractor\n",
    "import pickle\n",
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/fasttalk/lib/python3.11/site-packages/pytorch3d/io/obj_io.py:547: UserWarning: No mtl file provided\n",
      "  warnings.warn(\"No mtl file provided\")\n"
     ]
    }
   ],
   "source": [
    "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "flame    = FLAMEModel(n_shape=300,n_exp=50).to(device)\n",
    "renderer = Renderer(render_full_head=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_flatten_yaml(config_path):\n",
    "    \"\"\"\n",
    "    Loads the YAML file and flattens the structure so that\n",
    "    all sub-keys under top-level sections (e.g., DATA, NETWORK, etc.)\n",
    "    appear in a single dictionary without the top-level keys.\n",
    "    \"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        full_config = yaml.safe_load(f)\n",
    "\n",
    "    # Flatten the dict by merging all sub-dicts\n",
    "    flattened_config = {}\n",
    "    for top_level_key, sub_dict in full_config.items():\n",
    "        # sub_dict should itself be a dict of key-value pairs\n",
    "        if isinstance(sub_dict, dict):\n",
    "            # Merge each sub-key into flattened_config\n",
    "            for k, v in sub_dict.items():\n",
    "                flattened_config[k] = v\n",
    "        else:\n",
    "            # In case there's a non-dict top-level key (unlikely but possible)\n",
    "            flattened_config[top_level_key] = sub_dict\n",
    "\n",
    "    return SimpleNamespace(**flattened_config)\n",
    "\n",
    "# 1. Load YAML data into a Python dictionary\n",
    "global cfg\n",
    "cfg = load_and_flatten_yaml(\"config/talkinghead-1kh/demo.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(cfg)\n",
    "model = model.to(device)\n",
    "\n",
    "if os.path.isfile(cfg.model_path):\n",
    "    print(\"=> loading checkpoint '{}'\".format(cfg.model_path))\n",
    "    checkpoint = torch.load(cfg.model_path, map_location=lambda storage, loc: storage.cpu())\n",
    "    load_state_dict(model, checkpoint['state_dict'], strict=False)\n",
    "    print(\"=> loaded checkpoint '{}'\".format(cfg.model_path))\n",
    "else:\n",
    "    raise RuntimeError(\"=> no checkpoint flound at '{}'\".format(cfg.model_path))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "checkpoint = torch.load(cfg.model_path, map_location=lambda storage, loc: storage.cpu())\n",
    "\n",
    "audio_dir   = \"demo/input\"\n",
    "encoded_dir = \"demo/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample style\n",
    "style_sample_path = \"/root/Datasets/joint_data/npz/oonc4u-Adbc_0001_S366_E617_L271_T84_R735_B548.npz\"\n",
    "flame_param       = np.load(style_sample_path, allow_pickle=True)\n",
    "\n",
    "if 'pose' in flame_param:\n",
    "    expr   = flame_param[\"exp\"].reshape(-1,50)\n",
    "    jaw    = flame_param[\"pose\"][:,3:6].reshape(-1,3)\n",
    "    gpose  = flame_param[\"pose\"][:,0:3].reshape(-1,3)\n",
    "    gpose  = gpose - gpose.mean(axis=0, keepdims=True)\n",
    "elif 'pose_params' in flame_param:\n",
    "    expr   = flame_param['expression_params'].reshape(-1, 50)\n",
    "    jaw    = flame_param[\"jaw_params\"].reshape(-1, 3)\n",
    "    gpose  = flame_param[\"pose_params\"].reshape(-1, 3)\n",
    "    gpose  = gpose - gpose.mean(axis=0, keepdims=True)\n",
    "else:\n",
    "    expr    = flame_param[\"exp\"].reshape((flame_param[\"exp\"].shape[0], -1))\n",
    "    gpose   = flame_param[\"gpose\"].reshape((flame_param[\"gpose\"].shape[0], -1))\n",
    "    jaw     = flame_param[\"jaw\"].reshape((flame_param[\"jaw\"].shape[0], -1))\n",
    "\n",
    "# Apply Savitzky-Golay filter along the time axis for gpose (removes tracker's flickering) (axis=0)\n",
    "gpose = savgol_filter(gpose, window_length=7, polyorder=2, axis=0)\n",
    "eyelids = np.ones((expr.shape[0], 2))\n",
    "concat_blendshapes = np.concatenate((expr, gpose, jaw, eyelids), axis=1)\n",
    "style_tensor = torch.Tensor(concat_blendshapes).to(device='cuda').unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to eval and predict blenshapes from audio\n",
    "model.eval()\n",
    "\n",
    "for wav_file in glob.glob(os.path.join(audio_dir,\"*.wav\")):\n",
    "    print('Generating facial animation for {}...'.format(wav_file))\n",
    "    test_name  = os.path.basename(wav_file).split(\".\")[0]\n",
    "\n",
    "    predicted_blendhsapes_path = os.path.join(encoded_dir, test_name+'.npz')\n",
    "    speech_array, _ = librosa.load(wav_file, sr=16000)\n",
    "\n",
    "    # Use Wav2Vec audio features\n",
    "    processor = Wav2Vec2FeatureExtractor.from_pretrained(cfg.wav2vec2model_path)\n",
    "    print(cfg.wav2vec2model_path)\n",
    "\n",
    "    audio_feature = np.squeeze(processor(speech_array, sampling_rate=16000).input_values)\n",
    "    audio_feature = np.reshape(audio_feature, (-1, audio_feature.shape[0]))\n",
    "    audio_feature = torch.FloatTensor(audio_feature).to(device='cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        blendshapes_out = model.predict(audio_feature, target_style=style_tensor)\n",
    "        \n",
    "        exp_out, gpose_out, jaw_out, eyelids_out = torch.split(blendshapes_out, [50, 3, 3, 2], dim=-1)\n",
    "\n",
    "        exp_out, gpose_out, jaw_out, eyelids_out = exp_out.squeeze(1), gpose_out.squeeze(1), jaw_out.squeeze(1), eyelids_out.squeeze(1)\n",
    "\n",
    "        # Filter jitter\n",
    "        gpose_out =  torch.FloatTensor(savgol_filter(gpose_out.squeeze(0).cpu().numpy(), window_length=7, polyorder=2, axis=0)).unsqueeze(0)\n",
    "\n",
    "        print(f'Exp shape: {exp_out.shape}, Gpose shape: {gpose_out.shape}, Jaw shape: {jaw_out.shape}, Eyelids shape: {eyelids_out.shape}')\n",
    "\n",
    "        np.savez(predicted_blendhsapes_path, exp=exp_out.detach().cpu().numpy(), gpose=gpose_out.detach().cpu().numpy(), jaw=jaw_out.detach().cpu().numpy(), eyelids=eyelids_out.detach().cpu().numpy())\n",
    "        print(f'Save facial animation in {predicted_blendhsapes_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vertices_from_blendshapes(expr, gpose, jaw, eyelids):\n",
    "\n",
    "    # Load the encoded file\n",
    "    expr_tensor    = expr.to(device)\n",
    "    gpose_tensor   = gpose.to(device)\n",
    "    jaw_tensor     = jaw.to(device)\n",
    "    eyelids_tensor = eyelids.to(device)\n",
    "\n",
    "    target_shape_tensor = torch.zeros(expr_tensor.shape[0], 300).expand(expr_tensor.shape[0], -1).to(device)\n",
    "\n",
    "    I = matrix_to_euler_angles(torch.cat([torch.eye(3)[None]], dim=0),\"XYZ\").to(device)\n",
    "\n",
    "    eye_r    = I.clone().to(device).squeeze()\n",
    "    eye_l    = I.clone().to(device).squeeze()\n",
    "    eyes     = torch.cat([eye_r,eye_l],dim=0).expand(expr_tensor.shape[0], -1).to(device)\n",
    "\n",
    "    pose = torch.cat([gpose_tensor, jaw_tensor], dim=-1).to(device)\n",
    "\n",
    "    flame_output_only_shape,_ = flame.forward(shape_params=target_shape_tensor, \n",
    "                                              expression_params=expr_tensor, \n",
    "                                              pose_params=pose, \n",
    "                                              eye_pose_params=eyes)\n",
    "    return flame_output_only_shape.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(frame_inx, renderer_output_blendshapes, axes):\n",
    "    # Select the frames to plot\n",
    "    frame = renderer_output_blendshapes['rendered_img'][frame_inx].detach().cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "    # Update the second subplot\n",
    "    axes.clear()\n",
    "    axes.imshow((frame * 255).astype(np.uint8))\n",
    "    axes.axis('off')\n",
    "    axes.set_title(f'Frame Stage 1 (Blendshape) {frame_inx + 1}')\n",
    "\n",
    "# Function to create and save the video\n",
    "def create_and_save_video(encoded_dir,file_name, renderer,audio_dir,output_dir):\n",
    "    base_name = os.path.basename(file_name).replace('.npz', '')\n",
    "    print(base_name)\n",
    "    \n",
    "    blendshapes_data_encoded_exp    = np.load(f'{encoded_dir}/{base_name}.npz')['exp'].reshape(-1, 50)\n",
    "    blendshapes_data_encoded_gpose  = np.load(f'{encoded_dir}/{base_name}.npz')['gpose'].reshape(-1, 3)\n",
    "    blendshapes_data_encoded_jaw    = np.load(f'{encoded_dir}/{base_name}.npz')['jaw'].reshape(-1, 3)\n",
    "    blendshapes_data_encoded_eyelids = np.load(f'{encoded_dir}/{base_name}.npz')['eyelids'].reshape(-1, 2)\n",
    "\n",
    "    blendshapes_data_encoded_exp     = torch.tensor(blendshapes_data_encoded_exp, dtype=torch.float32).to(device)\n",
    "    blendshapes_data_encoded_gpose   = torch.tensor(blendshapes_data_encoded_gpose, dtype=torch.float32).to(device)\n",
    "    blendshapes_data_encoded_jaw     = torch.tensor(blendshapes_data_encoded_jaw, dtype=torch.float32).to(device)\n",
    "    blendshapes_data_encoded_eyelids = torch.tensor(blendshapes_data_encoded_eyelids, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Compute vertices from blendshapes\n",
    "    blendshapes_derived_vertices = get_vertices_from_blendshapes(blendshapes_data_encoded_exp,blendshapes_data_encoded_gpose, blendshapes_data_encoded_jaw, blendshapes_data_encoded_eyelids)\n",
    "    print(blendshapes_derived_vertices.shape)\n",
    "\n",
    "    # ==== Camera ====\n",
    "    cam = torch.tensor([5, 0, 0], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    cam = cam.expand(blendshapes_derived_vertices.shape[0], -1)\n",
    "\n",
    "    # Render the frames\n",
    "    renderer_output_blendshapes  = renderer.forward(blendshapes_derived_vertices, cam)\n",
    "\n",
    "    N = renderer_output_blendshapes['rendered_img'].shape[0] # Number of frames\n",
    "\n",
    "    # Create a figure with two subplots\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "    # Create an animation\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig, \n",
    "        update, \n",
    "        frames=N, \n",
    "        fargs=(renderer_output_blendshapes, axes),\n",
    "        interval=100\n",
    "    )\n",
    "\n",
    "    # Save the animation as a video file\n",
    "    video_file = f'{output_dir}/{base_name}.mp4'\n",
    "    ani.save(video_file, writer='ffmpeg', fps=25)\n",
    "    print(f\"Video saved as {video_file}\")\n",
    "    \n",
    "    # =============== Add audio to the video ===============\n",
    "    \n",
    "    # Add audio to the video\n",
    "    audio_file = f'{audio_dir}/{base_name}.wav'\n",
    "    output_with_audio = f'{output_dir}/{base_name}_new_with_audio.mp4'\n",
    "    if os.path.exists(audio_file):\n",
    "        cmd = f'ffmpeg -y -i {video_file} -i {audio_file} -c:v copy -c:a aac -strict experimental {output_with_audio}'\n",
    "        subprocess.run(cmd, shell=True)\n",
    "        print(f\"Video with audio saved as {output_with_audio}\")\n",
    "    else:\n",
    "        print(f\"Audio file {audio_file} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing encoded files\n",
    "encoded_dir = 'demo/output'\n",
    "audio_dir   = 'demo/input'\n",
    "output_dir  = 'demo/video'\n",
    "\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Directory created: {output_dir}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {output_dir}\")\n",
    "\n",
    "counter =  10\n",
    "# Iterate over all files in the encoded directory\n",
    "for file_name in os.listdir(encoded_dir):\n",
    "    if counter == 0:\n",
    "        break\n",
    "    if file_name.endswith('.npz'):\n",
    "        create_and_save_video(encoded_dir,file_name,renderer,audio_dir,output_dir)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fasttalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
