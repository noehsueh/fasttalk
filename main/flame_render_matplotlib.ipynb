{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# change base folder\n",
        "os.chdir('/mnt/fasttalk/')\n",
        "\n",
        "# --- Imports & paths ---\n",
        "from pathlib import Path\n",
        "import os, subprocess\n",
        "import numpy as np\n",
        "from scipy.signal import savgol_filter\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from flame_model.FLAME import FLAMEModel\n",
        "from renderer.renderer import Renderer\n",
        "import argparse\n",
        "import torch.nn.functional as F\n",
        "from pytorch3d.transforms import matrix_to_euler_angles\n",
        "\n",
        "\n",
        "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "flame    = FLAMEModel(n_shape=300,n_exp=50).to(device)\n",
        "renderer = Renderer(render_full_head=True).to(device)\n",
        "\n",
        "# >>> EDIT THESE <<<\n",
        "sequence_path = \"/mnt/smirk/results/npz/6iXGqEq_cpI_0062_S286_E323_L299_T143_R619_B463.npz\"\n",
        "audio_path    = sequence_path.replace(\"npz\", \"wav\")\n",
        "output_dir    = \"/mnt/fasttalk/demo/samples\"\n",
        "\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "base_name     = Path(sequence_path).stem\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Load NPZ, extract values, smooth pose ---\n",
        "flame_param = np.load(sequence_path, allow_pickle=True)\n",
        "\n",
        "for k in flame_param.keys():\n",
        "    print(f\"Key: {k}, Shape: {flame_param[k].shape}\")\n",
        "\n",
        "def extract_expr_jaw_gpose(arr):\n",
        "    \n",
        "    expr = arr['exp'].reshape(-1, 50)\n",
        "    jaw  = arr['jaw'].reshape(-1,3)\n",
        "    gpose= arr['pose'].reshape(-1, 3)\n",
        "    shape = arr['mica_shape'].reshape(1, -1)\n",
        "    eyelid = arr['eyes'].reshape(-1, 2)\n",
        "\n",
        "\n",
        "    gpose = gpose - gpose.mean(axis=0, keepdims=True)\n",
        "    return expr, jaw, gpose, shape, eyelid\n",
        "\n",
        "expr, jaw, gpose, shape, eyelid = extract_expr_jaw_gpose(flame_param)\n",
        "T = len(expr)\n",
        "if T >= 7:\n",
        "    gpose = savgol_filter(gpose, window_length=7, polyorder=2, axis=0)\n",
        "\n",
        "def get_fps(arr, default=25):\n",
        "    try: return int(arr.get('fps', default))\n",
        "    except Exception: return default\n",
        "fps = get_fps(flame_param, default=25)\n",
        "print(f\"Loaded: T={T}, fps={fps}, audio={'yes' if os.path.exists(audio_path) else 'no'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ca09396",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "353644b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_vertices_from_blendshapes(expr, gpose, jaw, shape, eyelids=None):\n",
        "\n",
        "    # Load the encoded file\n",
        "    expr_tensor    = expr.to(device)\n",
        "    gpose_tensor   = gpose.to(device)\n",
        "    jaw_tensor     = jaw.to(device)\n",
        "    \n",
        "    if eyelids is not None:\n",
        "        eyelids_tensor = eyelids.to(device)\n",
        "\n",
        "    target_shape_tensor = shape.expand(expr_tensor.shape[0], -1).to(device)\n",
        "\n",
        "    I = matrix_to_euler_angles(torch.cat([torch.eye(3)[None]], dim=0),\"XYZ\").to(device)\n",
        "\n",
        "    eye_r    = I.clone().to(device).squeeze()\n",
        "    eye_l    = I.clone().to(device).squeeze()\n",
        "    eyes     = torch.cat([eye_r,eye_l],dim=0).expand(expr_tensor.shape[0], -1).to(device)\n",
        "\n",
        "    pose = torch.cat([gpose_tensor, jaw_tensor], dim=-1).to(device)\n",
        "\n",
        "    flame_output_only_shape,_ = flame.forward(shape_params=target_shape_tensor, \n",
        "                                               expression_params=expr_tensor, \n",
        "                                               pose_params=pose, \n",
        "                                               eye_pose_params=eyes)\n",
        "    return flame_output_only_shape.detach()\n",
        "\n",
        "def update(frame_inx, renderer_output_blendshapes, axes):\n",
        "    # Select the frames to plot\n",
        "    frame = renderer_output_blendshapes['rendered_img'][frame_inx].detach().cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "    # Update the second subplot\n",
        "    axes.clear()\n",
        "    axes.imshow((frame * 255).astype(np.uint8))\n",
        "    axes.axis('off')\n",
        "    axes.set_title(f'Frame Stage 1 (Blendshape) {frame_inx + 1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Vertices/Camera (REPLACE with your real computation) ---\n",
        "## This is a stub placeholder — you must compute the vertices & camera as in your prior pipeline.\n",
        "## Example names below mirror your message: `blendshapes_derived_vertices`, `cam`.\n",
        "\n",
        "# raise RuntimeError(\"Replace this cell with your code that produces blendshapes_derived_vertices and cam.\")\n",
        "\n",
        "# Minimal placeholders so the notebook runs — replace with real data.\n",
        "H, W = 512, 512\n",
        "blendshapes_derived_vertices = get_vertices_from_blendshapes(torch.tensor(expr, dtype=torch.float32), \n",
        "                                                             torch.tensor(gpose, dtype=torch.float32), \n",
        "                                                             torch.tensor(jaw, dtype=torch.float32),\n",
        "                                                             torch.tensor(shape, dtype=torch.float32),\n",
        "                                                             torch.tensor(eyelid, dtype=torch.float32))\n",
        "\n",
        "cam = torch.tensor([5, 0, 0], dtype=torch.float32).unsqueeze(0).to(device)\n",
        "cam = cam.expand(blendshapes_derived_vertices.shape[0], -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98c305b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Render the frames\n",
        "renderer_output_blendshapes  = renderer.forward(blendshapes_derived_vertices, cam)\n",
        "\n",
        "N = renderer_output_blendshapes['rendered_img'].shape[0] # Number of frames\n",
        "\n",
        "# Create a figure with two subplots\n",
        "fig, axes = plt.subplots(1, 1, figsize=(5, 5))\n",
        "\n",
        "# Create an animation\n",
        "ani = animation.FuncAnimation(\n",
        "    fig, \n",
        "    update, \n",
        "    frames=N, \n",
        "    fargs=(renderer_output_blendshapes, axes),\n",
        "    interval=100\n",
        ")\n",
        "\n",
        "# Save the animation as a video file\n",
        "video_file = f'{output_dir}/{base_name}.mp4'\n",
        "ani.save(video_file, writer='ffmpeg', fps=25)\n",
        "print(f\"Video saved as {video_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============== Add audio to the video ===============\n",
        "\n",
        "# Add audio to the video\n",
        "output_with_audio = f'{output_dir}/{base_name}_with_audio.mp4'\n",
        "if os.path.exists(audio_path):\n",
        "    cmd = f'ffmpeg -y -i {video_file} -i {audio_path} -c:v copy -c:a aac -strict experimental {output_with_audio}'\n",
        "    subprocess.run(cmd, shell=True)\n",
        "    print(f\"Video with audio saved as {output_with_audio}\")\n",
        "else:\n",
        "    print(f\"Audio file {audio_path} not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31c61476",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0acbd481",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "fasttalk",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
